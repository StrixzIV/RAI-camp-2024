{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCyJQhzlN9DU"
      },
      "source": [
        "# Setting up\n",
        "\n",
        "Installing all essential packages and preprocess the video before feeding them to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN4HBWH42yfe"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4LBw6pI20ce"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import math\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Drag the recorded videos to the /datasets folder\n",
        "\n",
        "![](https://cdn.discordapp.com/attachments/888368041174384671/1307197937415229521/Screenshot_2024-11-15_125444.png?ex=67396e72&is=67381cf2&hm=27de2a2f2514c1c0a1942be9e861d67dfa9a14034dc1f0e27a1a5e72b826ab10&)\n",
        "\n",
        "Result\n",
        "\n",
        "![](https://cdn.discordapp.com/attachments/888368041174384671/1307197937658630184/Screenshot_2024-11-15_125520.png?ex=67396e72&is=67381cf2&hm=1ef8c6a23d1b0b49829b7a7c1cb3738c8a6f85e8fc91f8caf493fe1c358c9aee&)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIsLLeWoOuus"
      },
      "outputs": [],
      "source": [
        "os.makedirs(f\"/content/datasets/test\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Video Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daNqjV47eubU"
      },
      "outputs": [],
      "source": [
        "base_dir = '/content/datasets'\n",
        "target_duration = 10\n",
        "\n",
        "def normalize_videos(base_dir, target_duration):\n",
        "    video_files = [file for file in os.listdir(base_dir) if file.endswith('.mp4')]\n",
        "\n",
        "    for video_file in video_files:\n",
        "        video_path = os.path.join(base_dir, video_file)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        frame_interval = max(1, int(total_frames / (fps * target_duration)))\n",
        "\n",
        "        frames = []\n",
        "        frame_count = 0\n",
        "        success, frame = cap.read()\n",
        "        while success and frame_count < total_frames:\n",
        "            if frame_count % frame_interval == 0:\n",
        "                frames.append(frame)\n",
        "            success, frame = cap.read()\n",
        "            frame_count += 1\n",
        "\n",
        "        normalized_video_path = os.path.join(base_dir, video_file) \n",
        "        height, width, layers = frames[0].shape\n",
        "        out = cv2.VideoWriter(normalized_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "        for frame in frames:\n",
        "            out.write(frame)\n",
        "        out.release()\n",
        "        cap.release()\n",
        "\n",
        "        print(f\"Normalized video saved: {video_file}\")\n",
        "\n",
        "normalize_videos(base_dir, target_duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split Frames\n",
        "\n",
        "\n",
        "Extract frames from each video and also split into train and validate dataset to train the YOLO model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGDCYjvBP3FQ",
        "outputId": "4e99ed38-aec3-442e-cc65-8e9be633d139"
      },
      "outputs": [],
      "source": [
        "def create_output_directories(base_output_dir, video_name):\n",
        "    train_dir = os.path.join(base_output_dir, \"train\", video_name)\n",
        "    val_dir = os.path.join(base_output_dir, \"val\", video_name)\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "    return train_dir, val_dir\n",
        "\n",
        "# Function to save a single frame\n",
        "def save_frame(image, output_dir, frame_count):\n",
        "    file_name = os.path.join(output_dir, f\"{frame_count:04d}.png\")\n",
        "    cv2.imwrite(file_name, image)\n",
        "\n",
        "# Function to split frames into train and validation sets\n",
        "def split_frames(video_path, video_name, base_output_dir, frames_per_second=5, split_ratio=0.8, max_frames=20):\n",
        "    \n",
        "    train_dir, val_dir = create_output_directories(base_output_dir, video_name)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file: {video_name}\")\n",
        "        return\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_interval = max(1, int(fps / frames_per_second))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    total_extracted_frames = min(total_frames // frame_interval, max_frames)\n",
        "    train_frame_limit = math.ceil(total_extracted_frames * split_ratio)\n",
        "\n",
        "    print(f\"Processing video: {video_name}\")\n",
        "    print(f\"FPS: {fps}, Total frames: {total_frames}, Extracted frames: {total_extracted_frames}\")\n",
        "\n",
        "    count = 0\n",
        "    frame_number = 0\n",
        "    success, image = cap.read()\n",
        "    while success and count < total_extracted_frames:\n",
        "        if frame_number % frame_interval == 0:\n",
        "            output_dir = train_dir if count < train_frame_limit else val_dir\n",
        "            save_frame(image, output_dir, count)\n",
        "            count += 1\n",
        "        success, image = cap.read()\n",
        "        frame_number += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Frames split and saved for video: {video_name}\")\n",
        "\n",
        "base_dir = '/content/datasets'\n",
        "video_files = [file for file in os.listdir(base_dir) if file.endswith('.mp4')]\n",
        "\n",
        "for video_file in video_files:\n",
        "    video_path = os.path.join(base_dir, video_file)\n",
        "    video_name = os.path.splitext(video_file)[0]\n",
        "    split_frames(video_path, video_name, base_dir)\n",
        "\n",
        "print(\"Frame extraction and splitting completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzCuM1BeP6Wc"
      },
      "outputs": [],
      "source": [
        "def display_frames_row_format(base_output_dir, video_name, samples_per_folder=5):\n",
        "    fig, ax = plt.subplots(2, samples_per_folder + 1, figsize=(15, 5))\n",
        "    fig.suptitle(f\"Frames for Video: {video_name}\", fontsize=16)\n",
        "\n",
        "    ax[0, 0].text(0.5, 0.5, \"Train\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "    ax[0, 0].axis('off')\n",
        "    train_folder = os.path.join(base_output_dir, \"train\", video_name)\n",
        "    train_frames = [file for file in os.listdir(train_folder) if file.endswith('.png')][:samples_per_folder]\n",
        "\n",
        "    for i, frame_file in enumerate(train_frames):\n",
        "        frame_path = os.path.join(train_folder, frame_file)\n",
        "        image = cv2.imread(frame_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        ax[0, i + 1].imshow(image_rgb)\n",
        "        ax[0, i + 1].axis('off')\n",
        "\n",
        "    ax[1, 0].text(0.5, 0.5, \"Validation\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "    ax[1, 0].axis('off')\n",
        "    val_folder = os.path.join(base_output_dir, \"val\", video_name)\n",
        "    val_frames = [file for file in os.listdir(val_folder) if file.endswith('.png')][:samples_per_folder]\n",
        "\n",
        "    for i, frame_file in enumerate(val_frames):\n",
        "        frame_path = os.path.join(val_folder, frame_file)\n",
        "        image = cv2.imread(frame_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        ax[1, i + 1].imshow(image_rgb)\n",
        "        ax[1, i + 1].axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "for video_file in video_files:\n",
        "    video_name = os.path.splitext(video_file)[0]\n",
        "    display_frames_row_format(base_dir, video_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train the model\n",
        "\n",
        "Use the image data that we preprocess earlier to train the classificaiton algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meDx_ggVjUe7"
      },
      "outputs": [],
      "source": [
        "model = YOLO('yolov8n-cls.pt')\n",
        "model.train(data='/content/datasets', epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model evaluation\n",
        "\n",
        "Evaluating model performance by letting it predict the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix2OqN7qjVGU"
      },
      "outputs": [],
      "source": [
        "def perform_inference(model, image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    results = model(image_path)\n",
        "\n",
        "    predicted_class = results[0].names[results[0].probs.top1]\n",
        "    confidence = results[0].probs.top1conf.item()\n",
        "\n",
        "    display_image_with_prediction(image, predicted_class, confidence)\n",
        "\n",
        "def display_image_with_prediction(image, predicted_class, confidence):\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(image_rgb)\n",
        "    plt.title(f\"Prediction: {predicted_class}, Confidence: {confidence*100:.2f}%\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "HGj4xmEEjlCd",
        "outputId": "ae9b045e-6f20-49c4-9a10-2b59f7c8c316"
      },
      "outputs": [],
      "source": [
        "def process_images_in_folder(model, folder_path):\n",
        "    supported_extensions = ('.png', '.jpg')\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith(supported_extensions):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            print(f\"Processing image: {image_path}\")\n",
        "            perform_inference(model, image_path)\n",
        "\n",
        "test_folder_path = '/content/datasets/test'\n",
        "\n",
        "process_images_in_folder(model, test_folder_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
